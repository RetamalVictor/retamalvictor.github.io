/**
 * BPE Tokenizer for Browser
 *
 * Implements Byte-Pair Encoding compatible with HuggingFace tokenizers.
 */

export interface TokenizerConfig {
    vocab: Map<string, number>;
    merges: Array<[string, string]>;
    decoder: Map<number, string>;
    unkToken: string;
    unkId: number;
}

export class BPETokenizer {
    private vocab: Map<string, number>;
    private merges: Array<[string, string]>;
    private mergeRanks: Map<string, number>;
    private decoder: Map<number, string>;
    private unkId: number;

    private constructor(config: TokenizerConfig) {
        this.vocab = config.vocab;
        this.merges = config.merges;
        this.decoder = config.decoder;
        this.unkId = config.unkId;

        // Build merge ranks for efficient lookup
        this.mergeRanks = new Map();
        for (let i = 0; i < this.merges.length; i++) {
            const [a, b] = this.merges[i];
            this.mergeRanks.set(`${a} ${b}`, i);
        }
    }

    /**
     * Load tokenizer from HuggingFace JSON format.
     */
    static async fromUrl(url: string): Promise<BPETokenizer> {
        const response = await fetch(url);
        if (!response.ok) {
            throw new Error(`Failed to fetch tokenizer: ${response.statusText}`);
        }
        const data = await response.json();
        return BPETokenizer.fromJSON(data);
    }

    /**
     * Create tokenizer from parsed JSON.
     */
    static fromJSON(data: any): BPETokenizer {
        const model = data.model;
        if (!model || model.type !== 'BPE') {
            throw new Error('Expected BPE tokenizer');
        }

        // Build vocab map
        const vocab = new Map<string, number>();
        for (const [token, id] of Object.entries(model.vocab)) {
            vocab.set(token, id as number);
        }

        // Build decoder (id -> token)
        const decoder = new Map<number, string>();
        for (const [token, id] of vocab) {
            decoder.set(id, token);
        }

        // Parse merges - can be either ["a", "b"] arrays or "a b" strings
        const merges: Array<[string, string]> = [];
        for (const merge of model.merges || []) {
            if (Array.isArray(merge) && merge.length === 2) {
                // Already in [a, b] format
                merges.push([merge[0], merge[1]]);
            } else if (typeof merge === 'string') {
                // "a b" format - split on space
                const parts = merge.split(' ');
                if (parts.length === 2) {
                    merges.push([parts[0], parts[1]]);
                }
            }
        }

        // Get unknown token
        const unkToken = model.unk_token || '<unk>';
        const unkId = vocab.get(unkToken) ?? 0;

        return new BPETokenizer({
            vocab,
            merges,
            decoder,
            unkToken,
            unkId,
        });
    }

    /**
     * Get vocabulary size.
     */
    get vocabSize(): number {
        return this.vocab.size;
    }

    /**
     * Encode text to token IDs.
     */
    encode(text: string): number[] {
        if (text.length === 0) return [];

        // Split into words (simple whitespace-based for now)
        // HuggingFace uses Ġ prefix for tokens that follow whitespace
        const words = this.preTokenize(text);

        const tokens: number[] = [];
        for (const word of words) {
            const wordTokens = this.encodeWord(word);
            tokens.push(...wordTokens);
        }

        return tokens;
    }

    /**
     * Pre-tokenize: split on whitespace while preserving it as prefix.
     */
    private preTokenize(text: string): string[] {
        const words: string[] = [];
        let current = '';
        let atWordStart = true;

        for (let i = 0; i < text.length; i++) {
            const char = text[i];

            if (char === ' ' || char === '\n' || char === '\t') {
                if (current.length > 0) {
                    words.push(current);
                    current = '';
                }
                // Next non-space char will have Ġ prefix
                atWordStart = true;
            } else {
                if (atWordStart && words.length > 0) {
                    // Add Ġ prefix for words after whitespace
                    current = 'Ġ' + char;
                } else {
                    current += char;
                }
                atWordStart = false;
            }
        }

        if (current.length > 0) {
            words.push(current);
        }

        return words;
    }

    /**
     * Encode a single word using BPE.
     */
    private encodeWord(word: string): number[] {
        // Start with individual characters
        let tokens: string[] = Array.from(word);

        // Apply BPE merges
        while (tokens.length > 1) {
            // Find the merge with lowest rank
            let bestMergeIdx = -1;
            let bestRank = Infinity;

            for (let i = 0; i < tokens.length - 1; i++) {
                const pair = `${tokens[i]} ${tokens[i + 1]}`;
                const rank = this.mergeRanks.get(pair);
                if (rank !== undefined && rank < bestRank) {
                    bestRank = rank;
                    bestMergeIdx = i;
                }
            }

            if (bestMergeIdx === -1) {
                // No more merges possible
                break;
            }

            // Apply the merge
            const merged = tokens[bestMergeIdx] + tokens[bestMergeIdx + 1];
            tokens = [
                ...tokens.slice(0, bestMergeIdx),
                merged,
                ...tokens.slice(bestMergeIdx + 2),
            ];
        }

        // Convert tokens to IDs
        return tokens.map(t => this.vocab.get(t) ?? this.unkId);
    }

    /**
     * Decode token IDs to text.
     */
    decode(ids: number[]): string {
        const tokens = ids.map(id => this.decoder.get(id) ?? '');
        let text = tokens.join('');

        // Replace Ġ with space
        text = text.replace(/Ġ/g, ' ');

        return text;
    }

    /**
     * Decode a single token ID to string.
     */
    decodeToken(id: number): string {
        const token = this.decoder.get(id) ?? '';
        return token.replace(/Ġ/g, ' ');
    }

    /**
     * Get token ID for a single token string.
     */
    tokenToId(token: string): number {
        return this.vocab.get(token) ?? this.unkId;
    }

    /**
     * Get token string for an ID.
     */
    idToToken(id: number): string {
        return this.decoder.get(id) ?? '';
    }
}
