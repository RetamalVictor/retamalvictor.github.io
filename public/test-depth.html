<!DOCTYPE html>
<html>
<head>
    <title>ONNX Runtime Test</title>
    <style>
        body { font-family: monospace; padding: 20px; background: #1a1a2e; color: #eee; }
        .log { margin: 5px 0; }
        .success { color: #4ade80; }
        .error { color: #f87171; }
        .info { color: #60a5fa; }
    </style>
</head>
<body>
    <h2>ONNX Runtime Web Test</h2>
    <div id="output"></div>

    <!-- Load ONNX Runtime WASM-only bundle from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.17.0/dist/ort.wasm.min.js"></script>

    <script>
        const output = document.getElementById('output');

        function log(msg, type = 'info') {
            const div = document.createElement('div');
            div.className = `log ${type}`;
            div.textContent = msg;
            output.appendChild(div);
            console.log(msg);
        }

        async function test() {
            log('Starting ONNX Runtime Web test...');
            log(`ORT version: ${ort.env.versions.web}`);

            // Configure
            log('Configuring ONNX Runtime...');
            ort.env.wasm.wasmPaths = 'https://cdn.jsdelivr.net/npm/onnxruntime-web@1.17.0/dist/';
            ort.env.wasm.numThreads = 1;
            // Disable WebGPU to force pure WASM
            ort.env.webgpu.disabled = true;
            log('  wasmPaths: ' + ort.env.wasm.wasmPaths);
            log('  numThreads: ' + ort.env.wasm.numThreads);
            log('  webgpu.disabled: ' + ort.env.webgpu.disabled);

            try {
                log('Creating inference session...', 'info');
                const modelUrl = '/assets/models/depth/depth_pretrained.onnx';
                log('  Model URL: ' + modelUrl);

                const session = await ort.InferenceSession.create(modelUrl, {
                    executionProviders: ['wasm'],
                    graphOptimizationLevel: 'all'
                });

                log('Session created!', 'success');
                log(`  Inputs: ${session.inputNames.join(', ')}`);
                log(`  Outputs: ${session.outputNames.join(', ')}`);

                // Test inference
                log('Running inference with dummy input...');
                const inputSize = 384;
                const dummyData = new Float32Array(3 * inputSize * inputSize).fill(0.5);
                const inputTensor = new ort.Tensor('float32', dummyData, [1, 3, inputSize, inputSize]);

                const start = performance.now();
                const results = await session.run({ input: inputTensor });
                const latency = performance.now() - start;

                const outputTensor = results['depth'];
                log(`Inference completed in ${latency.toFixed(1)}ms`, 'success');
                log(`  Output shape: [${outputTensor.dims.join(', ')}]`);

                log('All tests passed!', 'success');

            } catch (error) {
                log('Error: ' + error.message, 'error');
                log('Stack: ' + error.stack, 'error');
            }
        }

        test();
    </script>
</body>
</html>
